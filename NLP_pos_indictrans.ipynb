{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe7d307e1eaa42f7be93f7355c2daddf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 13:22:23 INFO: Downloaded file to /home/interiit/stanza_resources/resources.json\n",
      "2024-11-22 13:22:23 INFO: Downloading default packages for language: hi (Hindi) ...\n",
      "2024-11-22 13:22:24 INFO: File exists: /home/interiit/stanza_resources/hi/default.zip\n",
      "2024-11-22 13:22:25 INFO: Finished downloading models and saved to /home/interiit/stanza_resources\n",
      "2024-11-22 13:22:25 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294d7a205b1341118932af14c479ff9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.9.0.json:   0%|   …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-22 13:22:25 INFO: Downloaded file to /home/interiit/stanza_resources/resources.json\n",
      "2024-11-22 13:22:26 INFO: Loading these models for language: hi (Hindi):\n",
      "===========================\n",
      "| Processor | Package     |\n",
      "---------------------------\n",
      "| tokenize  | hdtb        |\n",
      "| pos       | hdtb_charlm |\n",
      "===========================\n",
      "\n",
      "2024-11-22 13:22:26 INFO: Using device: cuda\n",
      "2024-11-22 13:22:26 INFO: Loading: tokenize\n",
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/stanza/models/tokenization/trainer.py:82: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-22 13:22:27 INFO: Loading: pos\n",
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/stanza/models/pos/trainer.py:139: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(filename, lambda storage, loc: storage)\n",
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/stanza/models/common/pretrain.py:56: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(self.filename, lambda storage, loc: storage)\n",
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/stanza/models/common/char_model.py:271: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state = torch.load(filename, lambda storage, loc: storage)\n",
      "2024-11-22 13:22:27 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "import stanza\n",
    "\n",
    "# stanza.download('hi')\n",
    "\n",
    "nlp = stanza.Pipeline(lang='hi', processors='tokenize,pos')\n",
    "text_tot = 'तीन छात्र समय सीमा से पहले सबमिशन करने के लिए कड़ी मेहनत कर रहे हैं'\n",
    "\n",
    "def pass_string(input):\n",
    "  lst = input\n",
    "  doc = nlp(lst)\n",
    "  words = doc.sentences[-1].words\n",
    "  n= len(words)\n",
    "  i= n-1\n",
    "  while(i):\n",
    "    if words[i].upos == 'NOUN' or words[i].upos == 'VERB':\n",
    "      return i\n",
    "    else:\n",
    "      pass\n",
    "    i-=1\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from IndicTransToolkit import IndicProcessor\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "class TransGen:\n",
    "  def __init__(self, translation_model = \"ai4bharat/indictrans2-indic-en-1B\",stable_diff_model = \"stabilityai/stable-diffusion-2-base\", src_lang = 'hin_Deva', tgt_lang = 'eng_Latn'):\n",
    "    self.bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(translation_model, trust_remote_code=True)\n",
    "    self.model = AutoModelForSeq2SeqLM.from_pretrained(translation_model, trust_remote_code=True, quantization_config = self.bnb_config)\n",
    "    self.ip = IndicProcessor(inference=True)\n",
    "    self.src_lang = src_lang\n",
    "    self.tgt_lang = tgt_lang\n",
    "\n",
    "  def translate(self, input_sentences):\n",
    "    batch = self.ip.preprocess_batch(\n",
    "        input_sentences,\n",
    "        src_lang=self.src_lang,\n",
    "        tgt_lang=self.tgt_lang,\n",
    "    )\n",
    "    inputs = self.tokenizer(\n",
    "        batch,\n",
    "        truncation=True,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "      generated_tokens = self.model.generate(\n",
    "          **inputs,\n",
    "          use_cache=True,\n",
    "          min_length=0,\n",
    "          max_length=256,\n",
    "          num_beams=5,\n",
    "          num_return_sequences=1,\n",
    "      )\n",
    "\n",
    "    with self.tokenizer.as_target_tokenizer():\n",
    "        generated_tokens = self.tokenizer.batch_decode(\n",
    "            generated_tokens.detach().cpu().tolist(),\n",
    "            skip_special_tokens=True,\n",
    "            clean_up_tokenization_spaces=True,\n",
    "        )\n",
    "\n",
    "    translations = self.ip.postprocess_batch(generated_tokens, lang=self.tgt_lang)\n",
    "\n",
    "    return translations\n",
    "\n",
    "transgen = TransGen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:452: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n",
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Three students are working hard to submit before the deadline.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transgen.translate(['तीन छात्र समय सीमा से पहले सबमिशन करने के लिए कड़ी मेहनत कर रहे हैं'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/transformers/generation/utils.py:2097: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n",
      "/home/interiit/miniconda3/envs/hp3/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:4114: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['There are three students']\n",
      "तीन छात्र  \n",
      "1 4\n",
      "['Submission before the deadline']\n",
      "समय सीमा से पहले सबमिशन  \n",
      "4 5\n",
      "['to work hard to make']\n",
      "करने के लिए कड़ी मेहनत कर  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "cur_sent = ''\n",
    "prev_idx = 0\n",
    "translation = ''\n",
    "for word in text_tot.split():\n",
    "    cur_sent += word + ' '\n",
    "    str_idx = pass_string(cur_sent)\n",
    "    if str_idx != 0 and str_idx != prev_idx:\n",
    "        print(prev_idx, str_idx)\n",
    "        prev_idx  = str_idx\n",
    "        print(transgen.translate([cur_sent]))# + ' '\n",
    "        print(cur_sent, translation)\n",
    "        cur_sent= \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prompt_template' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mSiLLM\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Prompter\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PeftModel\n\u001b[0;32m----> 5\u001b[0m prompter \u001b[38;5;241m=\u001b[39m Prompter(\u001b[43mprompt_template\u001b[49m)\n\u001b[1;32m      6\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m LlamaForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      9\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-1B-Instruct\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     10\u001b[0m             load_in_8bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     11\u001b[0m             torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     12\u001b[0m             device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     13\u001b[0m         )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prompt_template' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from SiLLM.utils.prompter import Prompter\n",
    "from peft import PeftModel\n",
    "\n",
    "prompter = Prompter(prompt_template)\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct')\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "            'meta-llama/Llama-3.2-1B-Instruct',\n",
    "            load_in_8bit=True,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "model = PeftModel.from_pretrained(\n",
    "    model,\n",
    "    'SiLLM/weights',\n",
    "    torch_dtype=torch.float16,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
